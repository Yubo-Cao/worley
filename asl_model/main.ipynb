{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "def preprocess(df) -> tuple[np.ndarray, np.ndarray]:\n",
    "    gloss, text = df[\"gloss\"].values, df[\"text\"].values\n",
    "    return text, gloss # x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_x, train_y = preprocess(train)\n",
    "test_x, test_y = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(df)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "test_raw = (\n",
    "    tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
    "    text = tf.strings.lower(text)\n",
    "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n",
    "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
    "    text = tf.strings.strip(text)\n",
    "    text = tf.strings.join([\"[START]\", text, \"[END]\"], separator=\" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'after a previous tragedy in belgium in 2001 , comprehensive safety measures were promised but have not been implemented .\\r\\n'\n",
      " b'under the pressure of this procedure , the italian authorities are now changing their approach .\\r\\n'\n",
      " b'europeans often ask what we do here in the european parliament , what good we do for them .\\r\\n'\n",
      " b\"to date , the commission has still not given clear responses to parliament's requests .\\r\\n\"\n",
      " b'unfortunately , the commission is very reluctant to suggest any measures in this field .\\r\\n'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'AFTER DESC-PREVIOUS TRAGEDY IN BELGIUM IN 2001 , DESC-COMPREHENSIVE SAFETY MEASURE BE PROMISE BUT HAVE DESC-NOT BE IMPLEMENT .\\r\\n'\n",
      " b'UNDER PRESSURE THIS PROCEDURE , ITALIAN AUTHORITY BE DESC-NOW CHANGE X-Y APPROACH .\\r\\n'\n",
      " b'EUROPEAN DESC-OFTEN ASK WHAT X-WE DO DESC-HERE IN EUROPEAN PARLIAMENT , WHAT DESC-GOOD X-WE DO FOR X-Y .\\r\\n'\n",
      " b'TO DATE , COMMISSION HAVE DESC-STILL DESC-NOT GIVE DESC-CLEAR RESPONSE TO PARLIAMENT X-POSS REQUEST .\\r\\n'\n",
      " b'DESC-UNFORTUNATELY , COMMISSION BE DESC-VERY DESC-RELUCTANT TO SUGGEST ANY MEASURE IN THIS FIELD .\\r\\n'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_context_strings, example_target_strings in train_raw.take(1):\n",
    "    print(example_context_strings[:5])\n",
    "    print()\n",
    "    print(example_target_strings[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "context_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True,\n",
    ")\n",
    "context_text_processor.adapt(train_raw.map(lambda context, _: context))\n",
    "\n",
    "target_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size,\n",
    "    ragged=True,\n",
    ")\n",
    "target_text_processor.adapt(train_raw.map(lambda _, target: target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(context, target):\n",
    "    context = context_text_processor(context).to_tensor()\n",
    "    target = target_text_processor(target)\n",
    "    targ_in = target[:, :-1].to_tensor()\n",
    "    targ_out = target[:, 1:].to_tensor()\n",
    "    return (context, targ_in), targ_out\n",
    "\n",
    "\n",
    "train_ds = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
    "val_ds = test_raw.map(process_text, tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.units = units\n",
    "\n",
    "        # The embedding layer converts tokens to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            self.vocab_size, units, mask_zero=True\n",
    "        )\n",
    "\n",
    "        # The RNN layer processes those vectors sequentially.\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            merge_mode=\"sum\",\n",
    "            layer=tf.keras.layers.GRU(\n",
    "                units,\n",
    "                # Return the sequence and state\n",
    "                return_sequences=True,\n",
    "                recurrent_initializer=\"glorot_uniform\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        # shape_checker = ShapeChecker()\n",
    "        # shape_checker(x, \"batch s\")\n",
    "\n",
    "        # 2. The embedding layer looks up the embedding vector for each token.\n",
    "        x = self.embedding(x)\n",
    "        # shape_checker(x, \"batch s units\")\n",
    "\n",
    "        # 3. The GRU processes the sequence of embeddings.\n",
    "        x = self.rnn(x)\n",
    "        # shape_checker(x, \"batch s units\")\n",
    "\n",
    "        # 4. Returns the new sequence of embeddings.\n",
    "        return x\n",
    "\n",
    "    def convert_input(self, texts):\n",
    "        texts = tf.convert_to_tensor(texts)\n",
    "        if len(texts.shape) == 0:\n",
    "            texts = tf.convert_to_tensor(texts)[tf.newaxis]\n",
    "        context = self.text_processor(texts).to_tensor()\n",
    "        context = self(context)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(\n",
    "            key_dim=units, num_heads=1, **kwargs\n",
    "        )\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # shape_checker = ShapeChecker()\n",
    "\n",
    "        # shape_checker(x, \"batch t units\")\n",
    "        # shape_checker(context, \"batch s units\")\n",
    "\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            value=context,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "\n",
    "        # shape_checker(x, \"batch t units\")\n",
    "        # shape_checker(attn_scores, \"batch heads t s\")\n",
    "\n",
    "        # Cache the attention scores for plotting later.\n",
    "        attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
    "        # shape_checker(attn_scores, \"batch t s\")\n",
    "        self.last_attention_weights = attn_scores\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, text_processor, units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.text_processor = text_processor\n",
    "        self.vocab_size = text_processor.vocabulary_size()\n",
    "        self.word_to_id = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(), mask_token=\"\", oov_token=\"[UNK]\"\n",
    "        )\n",
    "        self.id_to_word = tf.keras.layers.StringLookup(\n",
    "            vocabulary=text_processor.get_vocabulary(),\n",
    "            mask_token=\"\",\n",
    "            oov_token=\"[UNK]\",\n",
    "            invert=True,\n",
    "        )\n",
    "        self.start_token = self.word_to_id(\"[START]\")\n",
    "        self.end_token = self.word_to_id(\"[END]\")\n",
    "\n",
    "        self.units = units\n",
    "\n",
    "        # 1. The embedding layer converts token IDs to vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            self.vocab_size,\n",
    "            units,\n",
    "            mask_zero=True,\n",
    "        )\n",
    "\n",
    "        # 2. The RNN keeps track of what's been generated so far.\n",
    "        self.rnn = tf.keras.layers.GRU(\n",
    "            units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        )\n",
    "\n",
    "        # 3. The RNN output will be the query for the attention layer.\n",
    "        self.attention = CrossAttention(units)\n",
    "\n",
    "        # 4. This fully connected layer produces the logits for each\n",
    "        # output token.\n",
    "        self.output_layer = tf.keras.layers.Dense(self.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def call(self, context, x, state=None, return_state=False):\n",
    "    # shape_checker = ShapeChecker()\n",
    "    # shape_checker(x, \"batch t\")\n",
    "    # shape_checker(context, \"batch s units\")\n",
    "\n",
    "    # 1. Lookup the embeddings\n",
    "    x = self.embedding(x)\n",
    "    # shape_checker(x, \"batch t units\")\n",
    "\n",
    "    # 2. Process the target sequence.\n",
    "    x, state = self.rnn(x, initial_state=state)\n",
    "    # shape_checker(x, \"batch t units\")\n",
    "\n",
    "    # 3. Use the RNN output as the query for the attention over the context.\n",
    "    x = self.attention(x, context)\n",
    "    self.last_attention_weights = self.attention.last_attention_weights\n",
    "    # shape_checker(x, \"batch t units\")\n",
    "    # shape_checker(self.last_attention_weights, \"batch t s\")\n",
    "\n",
    "    # Step 4. Generate logit predictions for the next token.\n",
    "    logits = self.output_layer(x)\n",
    "    # shape_checker(logits, \"batch t target_vocab_size\")\n",
    "\n",
    "    if return_state:\n",
    "        return logits, state\n",
    "    else:\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_initial_state(self, context):\n",
    "    batch_size = tf.shape(context)[0]\n",
    "    start_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "    embedded = self.embedding(start_tokens)\n",
    "    return start_tokens, done, self.rnn.get_initial_state(embedded)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def tokens_to_text(self, tokens):\n",
    "    words = self.id_to_word(tokens)\n",
    "    result = tf.strings.reduce_join(words, axis=-1, separator=\" \")\n",
    "    result = tf.strings.regex_replace(result, \"^ *\\[START\\] *\", \"\")\n",
    "    result = tf.strings.regex_replace(result, \" *\\[END\\] *$\", \"\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Decoder.add_method\n",
    "def get_next_token(self, context, next_token, done, state, temperature=0.0):\n",
    "    logits, state = self(context, next_token, state=state, return_state=True)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "        next_token = tf.argmax(logits, axis=-1)\n",
    "    else:\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        next_token = tf.random.categorical(logits, num_samples=1)\n",
    "\n",
    "    # If a sequence produces an `end_token`, set it `done`\n",
    "    done = done | (next_token == self.end_token)\n",
    "    # Once a sequence is done it only produces 0-padding.\n",
    "    next_token = tf.where(done, tf.constant(0, dtype=tf.int64), next_token)\n",
    "\n",
    "    return next_token, done, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.keras.Model):\n",
    "    @classmethod\n",
    "    def add_method(cls, fun):\n",
    "        setattr(cls, fun.__name__, fun)\n",
    "        return fun\n",
    "\n",
    "    def __init__(self, units, context_text_processor, target_text_processor):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(context_text_processor, units)\n",
    "        decoder = Decoder(target_text_processor, units)\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        context, x = inputs\n",
    "        context = self.encoder(context)\n",
    "        logits = self.decoder(context, x)\n",
    "\n",
    "        # TODO(b/250038731): remove this\n",
    "        try:\n",
    "            # Delete the keras mask, so keras doesn't scale the loss+accuracy.\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNITS = 256\n",
    "model = Translator(UNITS, context_text_processor, target_text_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction=\"none\"\n",
    "    )\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def masked_acc(y_true, y_pred):\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "\n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_sum(match) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=masked_loss, metrics=[masked_acc, masked_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 64s 506ms/step - loss: 5.5593 - masked_acc: 0.1953 - masked_loss: 5.5593 - val_loss: 4.6287 - val_masked_acc: 0.3162 - val_masked_loss: 4.6287\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 74s 741ms/step - loss: 3.8929 - masked_acc: 0.4303 - masked_loss: 3.8929 - val_loss: 3.0121 - val_masked_acc: 0.5629 - val_masked_loss: 3.0121\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 65s 647ms/step - loss: 2.2858 - masked_acc: 0.6665 - masked_loss: 2.2858 - val_loss: 1.5158 - val_masked_acc: 0.7881 - val_masked_loss: 1.5158\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 71s 712ms/step - loss: 1.1900 - masked_acc: 0.8385 - masked_loss: 1.1900 - val_loss: 0.8865 - val_masked_acc: 0.8878 - val_masked_loss: 0.8865\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 56s 554ms/step - loss: 0.7656 - masked_acc: 0.9015 - masked_loss: 0.7656 - val_loss: 1.3131 - val_masked_acc: 0.8044 - val_masked_loss: 1.3131\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 40s 399ms/step - loss: 0.6496 - masked_acc: 0.9164 - masked_loss: 0.6496 - val_loss: 0.5699 - val_masked_acc: 0.9275 - val_masked_loss: 0.5699\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 62s 620ms/step - loss: 0.6502 - masked_acc: 0.9108 - masked_loss: 0.6502 - val_loss: 0.4858 - val_masked_acc: 0.9394 - val_masked_loss: 0.4858\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 75s 753ms/step - loss: 0.4261 - masked_acc: 0.9499 - masked_loss: 0.4261 - val_loss: 0.3934 - val_masked_acc: 0.9528 - val_masked_loss: 0.3934\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 57s 569ms/step - loss: 0.3862 - masked_acc: 0.9552 - masked_loss: 0.3862 - val_loss: 0.3937 - val_masked_acc: 0.9546 - val_masked_loss: 0.3937\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 50s 500ms/step - loss: 0.3532 - masked_acc: 0.9580 - masked_loss: 0.3532 - val_loss: 0.3487 - val_masked_acc: 0.9602 - val_masked_loss: 0.3487\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 51s 509ms/step - loss: 0.3427 - masked_acc: 0.9575 - masked_loss: 0.3427 - val_loss: 0.2941 - val_masked_acc: 0.9677 - val_masked_loss: 0.2941\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 51s 509ms/step - loss: 0.2996 - masked_acc: 0.9645 - masked_loss: 0.2996 - val_loss: 0.3521 - val_masked_acc: 0.9593 - val_masked_loss: 0.3521\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 49s 488ms/step - loss: 0.2957 - masked_acc: 0.9636 - masked_loss: 0.2957 - val_loss: 0.3074 - val_masked_acc: 0.9645 - val_masked_loss: 0.3074\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 50s 502ms/step - loss: 0.2815 - masked_acc: 0.9651 - masked_loss: 0.2815 - val_loss: 0.3612 - val_masked_acc: 0.9597 - val_masked_loss: 0.3612\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds.repeat(),\n",
    "    epochs=100,\n",
    "    steps_per_epoch=100,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=20,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Translator.add_method\n",
    "def translate(self, texts, *, max_length=50, temperature=0.0):\n",
    "    # Process the input texts\n",
    "    context = self.encoder.convert_input(texts)\n",
    "    batch_size = tf.shape(texts)[0]\n",
    "\n",
    "    # Setup the loop inputs\n",
    "    tokens = []\n",
    "    attention_weights = []\n",
    "    next_token, done, state = self.decoder.get_initial_state(context)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Generate the next token\n",
    "        next_token, done, state = self.decoder.get_next_token(\n",
    "            context, next_token, done, state, temperature\n",
    "        )\n",
    "\n",
    "        # Collect the generated tokens\n",
    "        tokens.append(next_token)\n",
    "        attention_weights.append(self.decoder.last_attention_weights)\n",
    "\n",
    "        if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "            break\n",
    "\n",
    "    # Stack the lists of tokens and attention weights.\n",
    "    tokens = tf.concat(tokens, axis=-1)  # t*[(batch 1)] -> (batch, t)\n",
    "    self.last_attention_weights = tf.concat(\n",
    "        attention_weights, axis=1\n",
    "    )  # t*[(batch 1 s)] -> (batch, t s)\n",
    "\n",
    "    result = self.decoder.tokens_to_text(tokens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'descre be [UNK] '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.translate([\"there is an apple\"])  # Are you still home\n",
    "result[0].numpy().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, embedding_3_layer_call_fn, embedding_3_layer_call_and_return_conditional_losses, cross_attention_1_layer_call_fn while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "# save model to tflite\n",
    "model.save(\"translator.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
